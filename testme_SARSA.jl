# setup the problem
include("problemsetup.jl");

# -- Solve the problem BELOW ------------------------------------------------------------------------------------ #
startstate = (1,1); # start position
number_of_episodes = 1000; # how many times do we repeat the task?
number_of_iterations = 20; # problem horizon (how many moves)

# simulate -
my_œÄ = let

    # setup up the SARSA agent model -
    agent_model_SARSA = let

        # load -
        saved_data = load("QQL.jld2");
    
        Œ± = 0.7;  # learning rate
        Œ≥ = 0.95; # discount rate
        nstates = (number_of_rows*number_of_columns);
        Q = zeros(number_of_states,number_of_actions) # Hmmm. Could we do something better than this?
        #Q = saved_data["QQL"]; # use the Q-learning Q as a starting point

        agent_model = build(MySARSAAgentModel, (
            states = ùíÆ, # states (coordinates)
            actions = ùíú, # actions 
            Œ± = Œ±, # learning rate
            Œ≥ = Œ≥, # discount rate
            Q = Q,# Hmmm. Could we do something better than this?
            my_œÄ = policy(Q) # policy
        ));

        agent_model
    end

    # Train the SARSA agent model -
    coordinate = startstate;
    T = 1.0;
    for i ‚àà 1:number_of_episodes

        T = (0.99)*T; # decrease the temperature ... we get less random, each time we repeat the task
        
        # run an episode, and grab the Q
        result = simulate(agent_model_SARSA, world_model, coordinate, number_of_iterations, 
            œµ = T);
    
        # update the agent with the Q from the last episode, and try to refine this Q
        agent_model_SARSA.Q = result.Q;  # Analogy: practice make perfect ...
        agent_model_SARSA.my_œÄ = policy(agent_model_SARSA.Q); # update the policy
    end
    my_œÄ = agent_model_SARSA.my_œÄ; # return the policy
end;
# -- Solve the problem ABOVE ------------------------------------------------------------------------------------ #